---
title: "HW10"
author: "Minh Luc, Devin Pham, Kyle Moore"
date: "Friday of Week 10, 06/03/2022"
output: 
  pdf_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***
### We all contributed equally for this homework.

# Question 0 

### Member 1: 
* Name: Minh Luc
* Student ID: A17209607

### Member 2: 

* Name: Kyle Moore
* Student ID: A14271413

### Member 3: 

* Name: Devin Pham
* Student ID: A17198936

***

\newpage

# Question 1
    
* **(a)**

    ```{r}
    library(ISLR2)
    library(MASS)
    auto_data <- Auto
    
    mpg01 <- ifelse(auto_data$mpg > median(auto_data$mpg), 1, 0)
    
    auto_data <- data.frame(auto_data, mpg01)
    
    head(auto_data)
    ```
    
* **(b)**

    ```{r}
    cor(subset(auto_data, select = -c(name) ))
    
    par(mfrow=c(2,3))
    boxplot(auto_data$cylinders~auto_data$mpg01)
    boxplot(auto_data$displacement~auto_data$mpg01)
    boxplot(auto_data$horsepower~auto_data$mpg01)
    boxplot(auto_data$weight~auto_data$mpg01)
    boxplot(auto_data$acceleration~auto_data$mpg01)
    boxplot(auto_data$year~auto_data$mpg01)
    
    par(mfrow=c(2,3))
    plot(auto_data$cylinders~auto_data$mpg01)
    plot(auto_data$displacement~auto_data$mpg01)
    plot(auto_data$horsepower~auto_data$mpg01)
    plot(auto_data$weight~auto_data$mpg01)
    plot(auto_data$acceleration~auto_data$mpg01)
    plot(auto_data$year~auto_data$mpg01)
    ```
    - The boxplots with the most separation appear to be cylinders, displacement, horsepower, and weight. Acceleration the least useful and year is middling but does show some separation. Scatterplots show similar but less easy to see results. By then confirming with the correlation table, we can see that our intuitions are confirmed, and the order of the useful correlations are cylinders, weight, displacement, then horsepower. I will exclude year and origin in the models.
    
* **(c)**

    ```{r}
    set.seed(1)
    
    n <- nrow(auto_data)
    
    train_index <- sample(1:n, size = n / 2)
    
    train <- auto_data[train_index,]
    test <- auto_data[-train_index,]
    ```
    
* **(d)**

    ```{r}
    lda.fit <- lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = train)
    lda.fit
    
    lda.pred <- predict(lda.fit, test)
    lda.class <- lda.pred$class
    table(lda.class, test$mpg01)
    
    accuracy <- mean(lda.class == test$mpg01); accuracy
    
    1 - accuracy
    ```
    
  - Test error for LDA was 12.7551%
    
* **(e)**

    ```{r}
    qda.fit <- qda(mpg01 ~ cylinders + displacement + horsepower + weight, data = train)
    qda.fit
    
    qda.pred <- predict(qda.fit, test)
    qda.class <- qda.pred$class
    table(qda.class, test$mpg01)
    
    accuracy <- mean(qda.class == test$mpg01); accuracy
    
    1 - accuracy
    ```
    
  - Test error for QDA was 11.73469%
    
* **(f)**

    ```{r}
    glm.fit <- glm(mpg01 ~ cylinders + displacement + horsepower + weight, data = train, family = binomial)
    summary(glm.fit)
    
    glm.probs <- predict(glm.fit, newdata = test,type = "response")
    glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
    
    table(glm.pred, test$mpg01)
    
    accuracy <- mean(glm.pred == test$mpg01); accuracy
    
    1 - accuracy
    ```
    
    
  - Test error for logistic regression was 12.2449%
    
* **(g)**

    ```{r}
    summary(glm.fit)$coef
    ```
    
  - The $\beta_1$ coefficient of cylinders is zero, the test statistic(z value) is -0.0988, and the p-value is .9213. Since our p-value is very large, we do not reject the null, and that is why we cannot assume that $\beta_1$ is not 0.
    
***

\newpage
    
# Question 2
    
* **(a)**

    ```{r}
    oj_data <- OJ
    
    n <- nrow(oj_data)
    
    train_index <- sample(1:n, size = 800)
    
    train <- oj_data[train_index,]
    test <- oj_data[-train_index,]
    ```
    
* **(b)**

    ```{r}
    library(e1071)
    
    svm.fit <- svm(Purchase ~ ., data = train, kernel = "linear", cost = .01)
    
    summary(svm.fit)
    ```
    
    - Linearn kernel has a total of 433 support vectors, 216 from CH and 217 from MM
    
* **(c)**

    ```{r}
    train_pred <- predict(svm.fit, train)
    test_pred <- predict(svm.fit, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
* **(d)**

    ```{r}
    # tuning to select cost for linear SVM
    
    tune.out <- tune(svm, Purchase ~ ., data = train, kernel = "linear", 
                     ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
    
    summary(tune.out)
    
    bestmod <- tune.out$best.model
    summary(bestmod)
    ```
    
* **(e)**

    ```{r}
    train_pred <- predict(bestmod, train)
    test_pred <- predict(bestmod, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
* **(f)**

    ```{r}
    svm.fit <- svm(Purchase ~ ., data = train, kernel = "radial", cost = .01)
    
    summary(svm.fit)
    
    train_pred <- predict(svm.fit, train)
    test_pred <- predict(svm.fit, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
    - Radial basis kernel has a total of 623 support vectors, 313 from CH and 310 from MM
    
    ```{r}
    # tuning to select cost for radial basis SVM
    
    tune.out <- tune(svm, Purchase ~ ., data = train, kernel = "radial", 
                     ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
    
    summary(tune.out)
    
    bestmod <- tune.out$best.model
    summary(bestmod)
    
    train_pred <- predict(bestmod, train)
    test_pred <- predict(bestmod, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
    
* **(g)**

    ```{r}
    svm.fit <- svm(Purchase ~ ., data = train, kernel = "polynomial",
                   degree = 2, cost = .01)
    
    summary(svm.fit)
    
    train_pred <- predict(svm.fit, train)
    test_pred <- predict(svm.fit, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
    - Polynomial kernel has a total of 625 support vectors, 315 from CH and 310 from MM

    ```{r}
    # tuning to select cost for polynomial SVM
    
    tune.out <- tune(svm, Purchase ~ ., data = train, kernel = "polynomial", 
                     degree = 2, ranges = list(cost = c(0.01, 0.1, 1, 5, 10)))
    
    summary(tune.out)
    
    bestmod <- tune.out$best.model
    summary(bestmod)
    
    train_pred <- predict(bestmod, train)
    test_pred <- predict(bestmod, test)
    
    table(train_pred, train$Purchase)
    table(test_pred, test$Purchase)
    
    # print training accuracy
    train_accuracy <- mean(train_pred == train$Purchase); train_accuracy
    
    # print training error
    1 - train_accuracy
    
    # print test accuracy
    test_accuracy <- mean(test_pred == test$Purchase); test_accuracy
    
    # print test error
    1 - test_accuracy
    ```
    
* **(h)**

    - Linear
      - training error = 15.875%
      - testing error = 17.40741%
    - Radial
      - training error = 13.875%
      - testing error = 22.59259%
    - Polynomial
      - training error = 14.75%
      - testing error = 22.22222%
      
    - The models all performed similarly on training accuracy, ~14-16% error, but the linear kernel had ~5% less error than the polynomial or radial SVMs. Therefore the linear model had the best results on this data for generalizability.
    

***